{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCcw4JEmT1T_"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# Brain tumor 3D segmentation with MONAI\n",
        "\n",
        "This tutorial shows how to construct a training workflow of multi-labels segmentation task.\n",
        "\n",
        "And it contains below features:\n",
        "1. Transforms for dictionary format data.\n",
        "1. Define a new transform according to MONAI transform API.\n",
        "1. Load Nifti image with metadata, load a list of images and stack them.\n",
        "1. Randomly adjust intensity for data augmentation.\n",
        "1. Cache IO and transforms to accelerate training and validation.\n",
        "1. 3D SegResNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
        "1. Deterministic training for reproducibility.\n",
        "\n",
        "The dataset comes from http://medicaldecathlon.com/.  \n",
        "Target: Gliomas segmentation necrotic/active tumour and oedema  \n",
        "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
        "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
        "Source: BRATS 2016 and 2017 datasets.  \n",
        "Challenge: Complex and heterogeneously-located targets\n",
        "\n",
        "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right).\n",
        "(Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))\n",
        "\n",
        "![image](https://github.com/Project-MONAI/tutorials/blob/main/figures/brats_tasks.png?raw=1)\n",
        "\n",
        "The image patches show from left to right:\n",
        "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
        "1. the tumor core (red) visible in T2 (Fig.B).\n",
        "1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
        "1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue).\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gUv9VzJT1UG"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "w9h8KYxpT1UH",
        "outputId": "f3c23cc6-90bf-4723-d8f2-b2795761ea2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'monai'\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!python -c \"import onnxruntime\" || pip install -q onnxruntime\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rCFNKcqT1UK"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-gZTd9fJT1UL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from monai.apps import DecathlonDataset\n",
        "from monai.config import print_config\n",
        "from monai.data import DataLoader, decollate_batch\n",
        "from monai.handlers.utils import from_engine\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.nets import SegResNet\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    Activationsd,\n",
        "    AsDiscrete,\n",
        "    AsDiscreted,\n",
        "    Compose,\n",
        "    Invertd,\n",
        "    LoadImaged,\n",
        "    MapTransform,\n",
        "    NormalizeIntensityd,\n",
        "    Orientationd,\n",
        "    RandFlipd,\n",
        "    RandScaleIntensityd,\n",
        "    RandShiftIntensityd,\n",
        "    RandSpatialCropd,\n",
        "    Spacingd,\n",
        "    EnsureTyped,\n",
        "    EnsureChannelFirstd,\n",
        ")\n",
        "from monai.utils import set_determinism\n",
        "import onnxruntime\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7zK54Q0T1UN"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZuwKJRZPT1UO"
      },
      "outputs": [],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "if directory is not None:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uWPjApfT1UQ"
      },
      "source": [
        "## Set deterministic training for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9iA-7LdT1UQ"
      },
      "outputs": [],
      "source": [
        "set_determinism(seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTanIazYT1UR"
      },
      "source": [
        "## Define a new transform to convert brain tumor labels\n",
        "\n",
        "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57yBlGS_T1US"
      },
      "outputs": [],
      "source": [
        "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
        "    \"\"\"\n",
        "    Convert labels to multi channels based on brats classes:\n",
        "    label 1 is the peritumoral edema\n",
        "    label 2 is the GD-enhancing tumor\n",
        "    label 3 is the necrotic and non-enhancing tumor core\n",
        "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
        "    and ET (Enhancing tumor).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, data):\n",
        "        d = dict(data)\n",
        "        for key in self.keys:\n",
        "            result = []\n",
        "            # merge label 2 and label 3 to construct TC\n",
        "            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n",
        "            # merge labels 1, 2 and 3 to construct WT\n",
        "            result.append(torch.logical_or(torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1))\n",
        "            # label 2 is ET\n",
        "            result.append(d[key] == 2)\n",
        "            d[key] = torch.stack(result, axis=0).float()\n",
        "        return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BccZNXyST1US"
      },
      "source": [
        "## Setup transforms for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdShdxDiT1US"
      },
      "outputs": [],
      "source": [
        "train_transform = Compose(\n",
        "    [\n",
        "        # load 4 Nifti images and stack them together\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.0, 1.0, 1.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[224, 224, 144], random_size=False),\n",
        "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
        "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
        "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
        "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "    ]\n",
        ")\n",
        "val_transform = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.0, 1.0, 1.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjOLNN9GT1UT"
      },
      "source": [
        "## Quickly load data with DecathlonDataset\n",
        "\n",
        "Here we use `DecathlonDataset` to automatically download and extract the dataset.\n",
        "It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hlLcojmcT1UT"
      },
      "outputs": [],
      "source": [
        "# here we don't cache any data in case out of memory issue\n",
        "train_ds = DecathlonDataset(\n",
        "    root_dir=root_dir,\n",
        "    task=\"Task01_BrainTumour\",\n",
        "    transform=train_transform,\n",
        "    section=\"training\",\n",
        "    download=True,\n",
        "    cache_rate=0.0,\n",
        "    num_workers=4,\n",
        ")\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
        "val_ds = DecathlonDataset(\n",
        "    root_dir=root_dir,\n",
        "    task=\"Task01_BrainTumour\",\n",
        "    transform=val_transform,\n",
        "    section=\"validation\",\n",
        "    download=False,\n",
        "    cache_rate=0.0,\n",
        "    num_workers=4,\n",
        ")\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH3ZT6KZT1UU"
      },
      "source": [
        "## Check data shape and visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqxiveT1T1UU"
      },
      "outputs": [],
      "source": [
        "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
        "val_data_example = val_ds[2]\n",
        "print(f\"image shape: {val_data_example['image'].shape}\")\n",
        "plt.figure(\"image\", (24, 6))\n",
        "for i in range(4):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    plt.title(f\"image channel {i}\")\n",
        "    plt.imshow(val_data_example[\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")\n",
        "plt.show()\n",
        "# also visualize the 3 channels label corresponding to this image\n",
        "print(f\"label shape: {val_data_example['label'].shape}\")\n",
        "plt.figure(\"label\", (18, 6))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.title(f\"label channel {i}\")\n",
        "    plt.imshow(val_data_example[\"label\"][i, :, :, 60].detach().cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC-6jTcPT1UU"
      },
      "source": [
        "## Create Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOrK4FQ8T1UU"
      },
      "outputs": [],
      "source": [
        "max_epochs = 300\n",
        "val_interval = 1\n",
        "VAL_AMP = True\n",
        "\n",
        "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
        "device = torch.device(\"cuda:0\")\n",
        "model = SegResNet(\n",
        "    blocks_down=[1, 2, 2, 4],\n",
        "    blocks_up=[1, 1, 1],\n",
        "    init_filters=16,\n",
        "    in_channels=4,\n",
        "    out_channels=3,\n",
        "    dropout_prob=0.2,\n",
        ").to(device)\n",
        "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
        "\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
        "\n",
        "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
        "\n",
        "\n",
        "# define inference method\n",
        "def inference(input):\n",
        "    def _compute(input):\n",
        "        return sliding_window_inference(\n",
        "            inputs=input,\n",
        "            roi_size=(240, 240, 160),\n",
        "            sw_batch_size=1,\n",
        "            predictor=model,\n",
        "            overlap=0.5,\n",
        "        )\n",
        "\n",
        "    if VAL_AMP:\n",
        "        with torch.autocast(\"cuda\"):\n",
        "            return _compute(input)\n",
        "    else:\n",
        "        return _compute(input)\n",
        "\n",
        "\n",
        "# use amp to accelerate training\n",
        "scaler = torch.GradScaler(\"cuda\")\n",
        "# enable cuDNN benchmark\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok9iY2LdT1UV"
      },
      "source": [
        "## Execute a typical PyTorch training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "D3umcPQ7T1UV"
      },
      "outputs": [],
      "source": [
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "best_metrics_epochs_and_time = [[], [], []]\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "metric_values_tc = []\n",
        "metric_values_wt = []\n",
        "metric_values_et = []\n",
        "\n",
        "total_start = time.time()\n",
        "for epoch in range(max_epochs):\n",
        "    epoch_start = time.time()\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step_start = time.time()\n",
        "        step += 1\n",
        "        inputs, labels = (\n",
        "            batch_data[\"image\"].to(device),\n",
        "            batch_data[\"label\"].to(device),\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        with torch.autocast(\"cuda\"):\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        epoch_loss += loss.item()\n",
        "        print(\n",
        "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
        "            f\", train_loss: {loss.item():.4f}\"\n",
        "            f\", step time: {(time.time() - step_start):.4f}\"\n",
        "        )\n",
        "    lr_scheduler.step()\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_labels = (\n",
        "                    val_data[\"image\"].to(device),\n",
        "                    val_data[\"label\"].to(device),\n",
        "                )\n",
        "                val_outputs = inference(val_inputs)\n",
        "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
        "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "            metric = dice_metric.aggregate().item()\n",
        "            metric_values.append(metric)\n",
        "            metric_batch = dice_metric_batch.aggregate()\n",
        "            metric_tc = metric_batch[0].item()\n",
        "            metric_values_tc.append(metric_tc)\n",
        "            metric_wt = metric_batch[1].item()\n",
        "            metric_values_wt.append(metric_wt)\n",
        "            metric_et = metric_batch[2].item()\n",
        "            metric_values_et.append(metric_et)\n",
        "            dice_metric.reset()\n",
        "            dice_metric_batch.reset()\n",
        "\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                best_metrics_epochs_and_time[0].append(best_metric)\n",
        "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
        "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
        "                )\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
        "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
        "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
        "total_time = time.time() - total_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "P5w2PSD7T1UW"
      },
      "outputs": [],
      "source": [
        "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYbYkrqlT1UW"
      },
      "source": [
        "## Plot the loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNayrY--T1UW"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"red\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"green\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(\"train\", (18, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Val Mean Dice TC\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
        "y = metric_values_tc\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"blue\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Val Mean Dice WT\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
        "y = metric_values_wt\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"brown\")\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Val Mean Dice ET\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
        "y = metric_values_et\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"purple\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qXMWrFQT1UX"
      },
      "source": [
        "## Check best pytorch model output with the input image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmVL5bVUT1UY"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # select one image to evaluate and visualize the model output\n",
        "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
        "    roi_size = (128, 128, 64)\n",
        "    sw_batch_size = 4\n",
        "    val_output = inference(val_input)\n",
        "    val_output = post_trans(val_output[0])\n",
        "    plt.figure(\"image\", (24, 6))\n",
        "    for i in range(4):\n",
        "        plt.subplot(1, 4, i + 1)\n",
        "        plt.title(f\"image channel {i}\")\n",
        "        plt.imshow(val_ds[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
        "    plt.show()\n",
        "    # visualize the 3 channels label corresponding to this image\n",
        "    plt.figure(\"label\", (18, 6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(f\"label channel {i}\")\n",
        "        plt.imshow(val_ds[6][\"label\"][i, :, :, 70].detach().cpu())\n",
        "    plt.show()\n",
        "    # visualize the 3 channels model output corresponding to this image\n",
        "    plt.figure(\"output\", (18, 6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(f\"output channel {i}\")\n",
        "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZYXrQV6T1UY"
      },
      "source": [
        "## Evaluation on original image spacings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dq7LELcT1UZ"
      },
      "outputs": [],
      "source": [
        "val_org_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\"]),\n",
        "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
        "        Spacingd(keys=[\"image\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n",
        "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_org_ds = DecathlonDataset(\n",
        "    root_dir=root_dir,\n",
        "    task=\"Task01_BrainTumour\",\n",
        "    transform=val_org_transforms,\n",
        "    section=\"validation\",\n",
        "    download=False,\n",
        "    num_workers=4,\n",
        "    cache_num=0,\n",
        ")\n",
        "val_org_loader = DataLoader(val_org_ds, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "post_transforms = Compose(\n",
        "    [\n",
        "        Invertd(\n",
        "            keys=\"pred\",\n",
        "            transform=val_org_transforms,\n",
        "            orig_keys=\"image\",\n",
        "            meta_keys=\"pred_meta_dict\",\n",
        "            orig_meta_keys=\"image_meta_dict\",\n",
        "            meta_key_postfix=\"meta_dict\",\n",
        "            nearest_interp=False,\n",
        "            to_tensor=True,\n",
        "            device=\"cpu\",\n",
        "        ),\n",
        "        Activationsd(keys=\"pred\", sigmoid=True),\n",
        "        AsDiscreted(keys=\"pred\", threshold=0.5),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXmC0GEIT1UZ"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for val_data in val_org_loader:\n",
        "        val_inputs = val_data[\"image\"].to(device)\n",
        "        val_data[\"pred\"] = inference(val_inputs)\n",
        "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
        "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
        "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "    metric_org = dice_metric.aggregate().item()\n",
        "    metric_batch_org = dice_metric_batch.aggregate()\n",
        "\n",
        "    dice_metric.reset()\n",
        "    dice_metric_batch.reset()\n",
        "\n",
        "metric_tc, metric_wt, metric_et = metric_batch_org[0].item(), metric_batch_org[1].item(), metric_batch_org[2].item()\n",
        "\n",
        "print(\"Metric on original image spacing: \", metric_org)\n",
        "print(f\"metric_tc: {metric_tc:.4f}\")\n",
        "print(f\"metric_wt: {metric_wt:.4f}\")\n",
        "print(f\"metric_et: {metric_et:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZxr70LGT1UZ"
      },
      "source": [
        "## Convert torch to onnx model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39-unNHbT1Ua"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 4, 240, 240, 160).to(device)\n",
        "onnx_path = os.path.join(root_dir, \"best_metric_model.onnx\")\n",
        "torch.onnx.export(model, dummy_input, onnx_path, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCLpLsk_T1Ua"
      },
      "source": [
        "## Inference onnx model\n",
        "Here we change the model used by predictor to onnx_infer, both of which are used to obtain a tensor after the input has been reasoned by the neural network.\n",
        "\n",
        "Note: If the warning `pthread_setaffinity_np failed` appears when executing this cell, this is a known problem with the onnxruntime and does not affect the execution result. If you want to disable the warning, you can cancel the following comment to solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QbFxpGCT1Ua"
      },
      "outputs": [],
      "source": [
        "# Using the following program snippet will not affect the execution time.\n",
        "# options = ort.SessionOptions()\n",
        "# options.intra_op_num_threads = 1\n",
        "# options.inter_op_num_threads = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sULuxK4pT1Ub"
      },
      "outputs": [],
      "source": [
        "def onnx_infer(inputs):\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: inputs.cpu().numpy()}\n",
        "    ort_outs = ort_session.run(None, ort_inputs)\n",
        "    return torch.Tensor(ort_outs[0]).to(inputs.device)\n",
        "\n",
        "\n",
        "def predict(input):\n",
        "    def _compute(input):\n",
        "        return sliding_window_inference(\n",
        "            inputs=input,\n",
        "            roi_size=(240, 240, 160),\n",
        "            sw_batch_size=1,\n",
        "            predictor=onnx_infer,\n",
        "            overlap=0.5,\n",
        "        )\n",
        "\n",
        "    if VAL_AMP:\n",
        "        with torch.autocast(\"cuda\"):\n",
        "            return _compute(input)\n",
        "    else:\n",
        "        return _compute(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcGwTr3zT1Ub"
      },
      "outputs": [],
      "source": [
        "onnx_model_path = os.path.join(root_dir, \"best_metric_model.onnx\")\n",
        "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "\n",
        "for val_data in tqdm(val_loader, desc=\"Onnxruntime Inference Progress\"):\n",
        "    val_inputs, val_labels = (\n",
        "        val_data[\"image\"].to(device),\n",
        "        val_data[\"label\"].to(device),\n",
        "    )\n",
        "\n",
        "    ort_outs = predict(val_inputs)\n",
        "    val_outputs = post_trans(torch.Tensor(ort_outs[0]).to(device)).unsqueeze(0)\n",
        "\n",
        "    dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "    dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
        "onnx_metric = dice_metric.aggregate().item()\n",
        "onnx_metric_batch = dice_metric_batch.aggregate()\n",
        "onnx_metric_tc = onnx_metric_batch[0].item()\n",
        "onnx_metric_wt = onnx_metric_batch[1].item()\n",
        "onnx_metric_et = onnx_metric_batch[2].item()\n",
        "\n",
        "print(f\"onnx metric: {onnx_metric}\")\n",
        "print(f\"onnx_metric_tc: {onnx_metric_tc:.4f}\")\n",
        "print(f\"onnx_metric_wt: {onnx_metric_wt:.4f}\")\n",
        "print(f\"onnx_metric_et: {onnx_metric_et:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swz99MZKT1Ub"
      },
      "source": [
        "## Check best onnx model output with the input image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltq8Hqk8T1Ub"
      },
      "outputs": [],
      "source": [
        "onnx_model_path = os.path.join(root_dir, \"best_metric_model.onnx\")\n",
        "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # select one image to evaluate and visualize the model output\n",
        "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
        "    val_output = inference(val_input)\n",
        "    val_output = post_trans(val_output[0])\n",
        "    ort_output = predict(val_input)\n",
        "    ort_output = post_trans(torch.Tensor(ort_output[0]).to(device)).unsqueeze(0)\n",
        "    plt.figure(\"image\", (24, 6))\n",
        "    for i in range(4):\n",
        "        plt.subplot(1, 4, i + 1)\n",
        "        plt.title(f\"image channel {i}\")\n",
        "        plt.imshow(val_ds[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
        "    plt.show()\n",
        "    # visualize the 3 channels label corresponding to this image\n",
        "    plt.figure(\"label\", (18, 6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(f\"label channel {i}\")\n",
        "        plt.imshow(val_ds[6][\"label\"][i, :, :, 70].detach().cpu())\n",
        "    plt.show()\n",
        "    # visualize the 3 channels model output corresponding to this image\n",
        "    plt.figure(\"output\", (18, 6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(f\"pth output channel {i}\")\n",
        "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
        "    plt.show()\n",
        "    plt.figure(\"output\", (18, 6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(f\"onnx output channel {i}\")\n",
        "        plt.imshow(ort_output[0, i, :, :, 70].detach().cpu())\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5XK_4VHT1Uc"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LRbZjmqT1Uc"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}